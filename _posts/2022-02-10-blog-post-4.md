---
layout: post
title: Blog Post 4
---

In this blog post, I will implement a version of the 'spectral clustering' algorithm, used for classification tasks in machine learning.

![spec1.png](/images/spec1.png)

The task is to classify data points that resemble something like those above. Using k-Means, the simplest classification algorithm, we classify them as:

![spec2.png](/images/spec2.png)

However, by human eye, we see the classification should more so resemble this:

![spec3.png](/images/spec3.png)

# Construct a 'Similarity Matrix' A

We want an $$n$$x$$n$$ symmetric matrix A, where entry A[i,j] is 1 if points i and j are 'close' to each other, and 0 if they are not close to each other.

```python
from sklearn import metrics

A = metrics.pairwise_distances(X) #nxn matrix for distances
eps = 0.4
Eps = np.full((n,n), eps) #nxn matrix all with value eps
A = A < Eps #nxn boolean matrix, 1 if within eps, 0 otherwise
A = 1 * A #boolean to int conversion
np.fill_diagonal(A, val = 0) #change diagonal entries 1 to 0
```

Here X holds our data and is a $$n$$x$$2$$ numpy array, where each row is the coordinates of a data point.

pairwise_distances() returns a matrix for the (euclidean) distance between each of the points amoung all the points, resulting in an $$n$$x$$n$$ matrix.

Then we compare the matrix from pairwise_distances() to a threshhold value `eps 0.4`

Note: The diagonal entries are initially 1 because they are 0 distance within themselves, however we change them to 0.

Here's the first 10 rows and columns of `A`

```python
print(A[:10,:10])
```
```
[[0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 1 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 0 0 0 0 1 0]]
```

# The 'Binary Norm Cut' metric

We want to create a metric for how good any two clusters are-- we will call this the 'binary norm cut objective'.

This metric will have a nice, small value when:

1. the number of points in one cluster that are close to points in the other cluster is small

2. neither cluster is too small

So we define the binary norm cut objective as:

$$N_A(C_0, C_1) \equiv \operatorname{cut}(C_0, C_1) 
\left( \frac{1}{\operatorname{vol}(C_0)}+\frac{1}{\operatorname{vol}(C_1)} \right)$$

where $$C_0$$ and $$C_1$$ are our two clusters. $$\operatorname{cut}(C_0,C_1)$$ is a measure of the first criteria, and $$\left( \frac{1}{\operatorname{vol}(C_0)}+\frac{1}{\operatorname{vol}(C_1)} \right)$$ is a measure of the second criteria.

## The $$\operatorname{cut}(C_0,C_1)$$ function

```python
def cut(A,y):
    n = A.shape[0] #square matrix dimension
    count = 0      #initialize counter
    for i in np.arange(n): #iterate over all the pairs
        for j in np.arange(i,n): #start at i to only go over once
            if y[i] != y[j]: #checks if in different clusters
                count += A[i,j] #adds 1 if they are 'not close'
    return count
```

Mathematically $$\operatorname{cut}(C_0,C_1) = \sum_{i \in C_0, j \in C_1}a_{ij}$$

So we add the values of $$A$$ if the points $$i,j$$ are in different clusters, but for nice choice of clusters, we'll mostly be adding zero. So we want our cut to be small.

Here's the cut value for the true clusters:
```python
cut(A,y)
```
```
13
```
And the cut value for a random assignment to two clusters:
```python
y_random = np.random.randint(2, size = y.size)
cut(A, y_random)
```
```
1150
```

## The $$\operatorname{vol}(C)$$ function

The volume for a cluster will measure the points in that cluster that are 'close' to points in the other cluster.

Mathematically, $$\operatorname{vol}(C) = \sum_{i\in C} \sum_{j=1}^n a_{i,j}$$

So $$\left( \frac{1}{\operatorname{vol}(C_0)}+\frac{1}{\operatorname{vol}(C_1)} \right)$$, returns a high, bad score if either of the volumes are small.

```
def vols(A,y):
    v1 = np.sum(A @ y) #matrix multiplication to only 
                       #preserve values in cluster one
    v0 = np.sum(A) - v1 #noticing v0 + v1 = num of 1's in A
    return (v0, v1)
```

## The $$ \operatorname{normcut}(A,y)$$ function

Remember, we use our previous to functions to define the binary norm cut objective as:

$$N_A(C_0, C_1) \equiv \operatorname{cut}(C_0, C_1) 
\left( \frac{1}{\operatorname{vol}(C_0)}+\frac{1}{\operatorname{vol}(C_1)} \right)$$

So in python it becomes:

```python
def normcut(A,y):
    c = cut(A,y)
    v0, v1 = vols(A,y)
    nc = c*( (1/v0) + (1/v1) )
    return nc
```

Comparing the normcuts for the true clusters versus randomly assigned clusters:
```python
normcut(A,y), normcut(A, y_random)
```
```
(0.011518412331615225, 1.0240023597759158)
```

We see that the normcut for the true cluster is about 100 times smaller than randomly assigned clusters.

# Finding a way to Minimize the normcut

In reality, minimizing the norm cut may not be possible, so we will introduce some terms that is closely related to the norm cut.

Define a new vector $$z \in \mathbb{R}^n$$ where:

$$z_i = \begin{cases} \frac{1}{\operatorname{vol}(C_0)}, &\text{if } y_i = 0 \\
-\frac{1}{\operatorname{vol}(C_1)}, &\text{if } y_i = 1 \end{cases}$$

Then it turns out that:

$$N_A(C_0,C_1) = \frac{z^T(D-A)z}{z^TDz} $$

Where is a diagonal matrix where its diagonal entries are the row sums of its corresponding row in $$A$$.

In python, we can show that this equality holds within computer precision:

```python
def transform(A,y):
    v0, v1 = vols(A,y)
    z_1 = (-1/v1) * y
    z_0 = (1/v0) * (1-y) 
    return z_0 + z_1
```

```python
lhs = normcut(A,y)             #left hand side of equation
D = np.diag(A.sum(axis=1))     #diagonal entries are row sums from A     
z = transform(A,y)
rhs = (z @ (D-A) @ z) / (z @ D @ z) #right hand side of equation
np.isclose(lhs, rhs)           #check if equal up to computer precision
```
```
True
```

We also want to check $$z^TD\mathbb{1} = 0$$, where $$\mathbb{1}$$ is a vector of all ones

```python
np.allclose(z @ D @ np.ones(n), np.zeros(n))
```
```
True
```

# Minimizing $$\frac{z^T(D-A)z}{z^TDz}$$ through a work-around method

This minimizing task has the constrained condition that $$z^TD\mathbb{1} = 0$$, which is basically a condition that the elements of z have equal amounts of positive and negative values (which is that the cluster sizes are rougly equal)

However, if we substitute $$z$$ for $$z^*$$ where $$z^*$$ is the orthogonal complement of $$z$$ relative to $$D\mathbb{1}$$, then try to minimize $$\frac{z^{*T}(D-A)z^*}{z^{*T}Dz^*}$$, this will actually account for the constrained condition.

So we implement this function into python:
```python
def orth(u, v): #function to help to find orthogonal complement
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d) #is the orthogonal complement of z relative to d
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Utilize scipy library to find minimizer:
```python
from scipy.optimize import minimize
z_min = minimize(orth_obj, np.ones(n))
```

Observing the first few entries of our solution:
```
array([ 2.57173944,  2.65957691, -0.25564079, -0.11019831, -0.51459423])
```
We interpret this as negative values belong to cluster 1 and postive values belong to cluster 0.

# Results of optimizing the orthogonal objective

```python
X_C1 = X[z_min.x < 0] #negative belong to cluster 1
X_C0 = X[z_min.x >= 0]#postive belong to cluster 0
plt.scatter(X_C1[:,0], X_C1[:,1], c = 'gold')
plt.scatter(X_C0[:,0], X_C0[:,1], c = 'indigo')
```
![spec4.png](/images/spec4.png)

# Minimizing the orthogonal objective through a 'spectral' method

The most previous method to minimize is too slow and impractical. Our final iteration for this classification task will be to take the last method and equate it to solving for eigenvectors and eigenvalues, which is computationally easy to do!

So our task was to minimize:

$$\frac{z^T(D-A)z}{z^TDz}$$

with respect to $$z$$ with constraint $$z^TD\mathbb{1} = 0$$

According the The Rayleigh-Ritz Theorem, the solution $$z$$ to the above problem must be equal to the eigenvector with the smallest eigenvalue in the problem:

$$D^{-1}(D-A)z = \lambda z, z^T \mathbb{1} = 0$$

It turns out that $$\mathbb{1}$$ is the eigenvector with the smallest eigen value.

Therefore the $$z$$ that we want is the eigenvector with the second smallest eigenvalue.

To find $$z$$ in python using numpy linalg functions:

```python
L = np.linalg.inv(D) @ (D - A) #known in math as the laplacian of A

e_val, e_vec = np.linalg.eig(L)
index_for_second_smallest = np.argsort(e_val)[1]#argument sorting eigenvalues
z_eig = e_vec[:,index_for_second_smallest] #eigenvalue for second smallest
                                           #eigenvector
z_eig = z_eig.reshape(n) #helpful reshaping
```

Seeing the results of this new method:
```python
X_C1 = X[z_eig < 0]
X_C0 = X[z_eig >= 0]
plt.scatter(X_C1[:,0], X_C1[:,1], c = 'indigo')
plt.scatter(X_C0[:,0], X_C0[:,1], c = 'gold')
```

![spec5.png](/images/spec5.png)

Mathmatically, `z_eig` and `z_min` are proportionally to each other. But since we only care for the signs, the scaling doesn't matter.

We observe that our results are almost completely correct! there appears to be one mislabeled purple point in the yellow cluster.

# Synthesizing the Results

We will now construct a function to perform spectral clustering using our previous method
```python
def spectral_clustering(X, epsilon):
    """
    Compute two clusters for data points in X through the spectral
    clustering method. X is nx2 numpy array for n data points. epsilon
    is float value for the threshold distance when computing the
    similarity matrix. Output is numpy array of length n with values
    0 or 1 corresponding to the two clusters for the data points.
    """
    A = metrics.pairwise_distances(X) #nxn matrix for distances
    Eps = np.full((n,n), epsilon) #nxn matrix all with value eps
    A = (A < Eps) * 1 #nxn int matrix, 1 if within eps, 0 otherwise
    np.fill_diagonal(A, val = 0) #change diagonal entries 1 to 0
    
    D = np.diag(A.sum(axis=1)) #diagonal matrix where entries are row sums of A
    L = np.linalg.inv(D) @ (D - A) #Laplacian of A
    
    e_val, e_vec = np.linalg.eig(L) #eigenvalues and eigenvalues of L
    index_for_second_smallest = np.argsort(e_val)[1]
    z_eig = (e_vec[:,index_for_second_smallest]).reshape(n) #eigenvector with
                                                            #secondsmallest eigenvalue

    y = np.array([0 if z >= 0 else 1 for z in z_eig]) #labels based on sign of z_eig
    return y
```

A demonstration of the function:
```python
y_ = spectral_clustering(X, 0.4)
X_C1 = X[y_ == 1]
X_C0 = X[y_ == 0]
plt.scatter(X_C1[:,0], X_C1[:,1], c = 'gold')
plt.scatter(X_C0[:,0], X_C0[:,1], c = 'indigo')
```
![spec6.png](/images/spec6.png)

We can see after sythesizing into a function, we still observe that our algorithm performs really well! Still looks to be just one mislabeled point.

# Demonstrating on different Moons

We will create a new data set with more data points and the moons are less dense than before. How will our spectral clustering work on this?

Remember in our first data set that `noise = 0.05`

`noise = 0.1`
```python
np.random.seed(4444)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
fig = plt.scatter(X[:,0], X[:,1])

y_ = spectral_clustering(X, 0.4)
X_C1 = X[y_ == 1]
X_C0 = X[y_ == 0]
plt.scatter(X_C1[:,0], X_C1[:,1], c = 'gold')
plt.scatter(X_C0[:,0], X_C0[:,1], c = 'indigo')

plt.show()
```
![spec7.png](/images/spec7.png)

`noise = 0.15`
```python
np.random.seed(4444)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.5, random_state=None)
fig = plt.scatter(X[:,0], X[:,1])

y_ = spectral_clustering(X, 0.4)
X_C1 = X[y_ == 1]
X_C0 = X[y_ == 0]
plt.scatter(X_C1[:,0], X_C1[:,1], c = 'gold')
plt.scatter(X_C0[:,0], X_C0[:,1], c = 'indigo')

plt.show()
```
![spec8.png](/images/spec8.png)

`noise = 0.2`
```python
np.random.seed(4444)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.5, random_state=None)
fig = plt.scatter(X[:,0], X[:,1])

y_ = spectral_clustering(X, 0.4)
X_C1 = X[y_ == 1]
X_C0 = X[y_ == 0]
plt.scatter(X_C1[:,0], X_C1[:,1], c = 'gold')
plt.scatter(X_C0[:,0], X_C0[:,1], c = 'indigo')

plt.show()
```
![spec9.png](/images/spec9.png)

With increasing noise, we can see the spectral algorithm had a tougher time making clusters for each moon. However, to the human eye, seeing these blurred-moon clusters would also be difficult

# Demonstrating on different Bulls-Eye

This data set now has one cluster in the center of another cluster
```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![spec10.png](/images/spec10.png)

Here's the result when using the k-means algorithm:
```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![spec11.png](/images/spec11.png)

Again, kmeans does not perform well with this data set.

Now, with our spectral clustering algorithm:
```python
fig, ax = plt.subplots(5,2, figsize = (11,15))

for i in np.arange(10):
    eps = [0.11,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
    #for some reason eps = 0.1 produces linalg error, so using .11 instead
    row = i // 2
    col = i % 2
    y_ = spectral_clustering(X, eps[i])
    X_C1 = X[y_ == 1]
    X_C0 = X[y_ == 0]
    ax[row,col].scatter(X_C1[:,0], X_C1[:,1], c = 'indigo')
    ax[row,col].scatter(X_C0[:,0], X_C0[:,1], c = 'gold')
    ax[row,col].title.set_text(f"eps = {eps[i]}" )

plt.tight_layout()
```
![spec12.png](/images/spec12.png)

We see that our algorithm works on Bulls eye when eps = 0.3, 0.4, 0.5, which shows that's important to tinker with parameters when performing clustering tasks. In many of the other cases, it split the clusters along a line.

That concludes this blog on our journey to construct a classification algorithm!

