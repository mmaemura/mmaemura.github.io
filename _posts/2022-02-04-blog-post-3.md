---
layout: post
title: Blog Post 3
---

In this blog post I will demonstrate webscraping on IMDB.com to gather data on the movies played by certain actors. Then I will use the data to construct a pandas dataframe to display the movies that have the most actors in common. This gives me an idea of other movies I may like, since Cars is my favorite movie. 

# 1: Setup

I'll start by going to the IMDB page for my favorite movie. 

![cars1.png](/images/cars1.png)

Then I'll click on 'Cast & crew' which will take me to the page for actors in the movie.

![cars2.png](/images/cars2.png)

Scrolling down, I see this area with a list of the actors. Clicking on the picture of them, takes me to another page with information about the actor.

![cars3.png](/images/cars3.png)

Now I'm on this page from clicking Owen Wilson's face.

![cars4.png](/images/cars4.png)

Scrolling down I see a filmography of the other titles Owen Wilson has been apart of.

![cars5.png](/images/cars5.png)

## Goal: Use webscraping to create a csv, where the first column is actor/actress names from the movie Cars, and the second column is a list of the titles they have acted in.

# 2: Write the Scraper:

## Initialize:

Create a new GitHub repository for your scraping project. Open the terminal and enter:
```
conda activate PIC16B
scrapy startproject IMDB_scraper
cd IMDB_scraper
```
In settings.py, add the following line:
```
CLOSESPIDER_PAGECOUNT = 20
```
This'll limit the downloads of the scraper, while we still test things out.

## The scrapy.Spider class:

Create a new file inside the spiders folder, called 'imdb_spider.py'. All of the code we need will be in a class:
```python
import scrapy

class ImdbSpider(scrapy.Spider):
  """
  class to house our scraper. Will start on the IMDB page for 'Cars', then
  will crawl to the actors' page for the movie. Then will crawl to each of
  the actors and gather data on the movies they have all acted in. Yields
  a csv for all the actors and their movies.
  """

  name = 'imdb_spider' #will use this name when calling the spider

  start_urls = 'https://www.imdb.com/title/tt0317219/' #the 'Cars' IMDB page

  def parse(self, response):
    ...
  
  def parse_full_credits(self, response):
    ...

  def parse_actor_page(self, response):
    ...
```

## the parse() method:

The first method parse() will be the first one used by the spider. This method assumes we're on the main page for Cars, then will take use to the Cast & crew page.

![cars6.png](/images/cars6.png)


Opening DevTools on the chrome page for Cars and hovering over the 'Cast & crew' link, we get some insight to how our spider will work.

The link is under the class 'ipc-inline-list__item', and it is an 'a' with an 'href' that has the link.

```python
    def parse(self, response):
        """
        Assumes we're on the start url, i.e the main page for Cars on IMDB.
        Then takes traverses to the 'Cast & crew' page for Cars. 
        """

        cast_page_link = response.css("li.ipc-inline-list__item a")
          [2].attrib['href']
        #after testing, we need the second item from
        #response.css(li.ipc-inline-list__item a)

        if cast_page_link:
            cast_page_link = response.urljoin(cast_page_link)
            #creates the full url for the 'Cast & crew' page

            yield scrapy.Request(cast_page_link, 
              callback = self.parse_full_credits)
            #attempts to crawl to the 'Cast & crew' page
```
This code is short since we just have to follow one link, but finding the correct response.css() is crucial and will take some tinkering.

## the parse_full_credits() method:

Now inspecting the Cast page with DevTools and hovering over the picture for Owen Wilson, which is a link to his page,

![cars7.png](/images/cars7.png)
![cars8.png](/images/cars8.png)

We similarly look that the above class is 'primary_photo', and we want the 'href' from 'a' because we want to follow links.

```python
    def parse_full_credits(self, response):
        """
        Assumes we're on the 'Crew & cast' page for a movie.
        Will then crawl to the pages of actors listed.
        """
    
        actors_suffixes = [a.attrib["href"] for a in 
          response.css("td.primary_photo a")]
        #the partial urls for each of the actors, stored in a list

        prefix = "https://www.imdb.com/"
        #first portion of the complete url for the actors' page

        actors_urls = [prefix + suffix for suffix in actors_suffixes]
        #concatenates to produce the full url

        for url in actors_urls:
            yield scrapy.Request(url, callback = self.parse_actor_page) 
            #attempt to crawl to each of the actors' pages
```
The code is pretty short and similar to the parse() method, but this time we use a for loop to iterate over all the actors.

## the parse_actor_page() method:

First we want to extract the actors' names. We use DevTools to investigate...
![cars9.png](/images/cars9.png)
![cars10.png](/images/cars10.png)

We want to get the test here, and we notice the class 'itemprop' and that it's in a 'span'

Second, we want to construct a list of all the acting titles this actor has been in. Using DevTools, 

![cars11.png](/images/cars11.png)
![cars12.png](/images/cars12.png)

After experimenting around, we notice that 'response.css('div.filmo-row') will give us a list of all the items in the filmography section. Then for each of these we want to check if they're acting roles, and then get the text for the link of their titles. (but not the link url themselves)

```python
    def parse_actor_page(self, response):
        """
        Assumes we're on a actor's page on IMDB.
        Yields a dictionary where the first element is
        their name, and the second is a list of all
        their movies with a credited acting role
        """
        actor_name = response.css("span.itemprop::text")[0].get()
        #after testing, we need the 0th element from
        #response.css('span.itemprop::test')
        #use get() and ::text to extract just the text

        movie_or_TV_name = [] #initiate list for the names for actor's titles

        filmo_listings = response.css("div.filmo-row")
        #list for each item in the filmographing section

        for filmo in filmo_listings: #iterate over each credit
            role = filmo.css("::attr(id)").get()
            #extract the type of work, e.g actor/actress, producer,
            #writer, soundtrack, etc

            if role[0:3] == 'act': #check if 'actor' or 'actress'
                media_name = filmo.css("a::text")[0].get()
                #get the texts of links, after experimenting,
                #we want the 0th element

                media_name = media_name.replace(",", "")
                #remove commas because extracting to csv later on

                movie_or_TV_name.append(media_name)
                #add to the list

        yield { #final output
            "actor": actor_name, #string
            "movie_or_TV_name": movie_or_TV_name #list of strings
        }
```
This code is more invovled because we're extracting information now, instead of traversing links. We also have to iterate over titles, and check if they were acting roles. And then finally add a yield, which is the final output for our spider.

## Run the spider:
We go back to settings.py and comment out `CLOSESPIDER_PAGECOUNT = 20` because we're done testing.

Now in the console we run:
```
scrapy crawl imdb_spider -o results.csv
```
which outputs 'results.csv' in our directory. Here's what it looks like:

![cars13.png](/images/cars13.png)

And that completes the webscraping process, we will now move to a jupyter notebook to assess the results.

# 3: Find the Related Movies:

The packages I'll be using.
```python
import pandas as pd
from matplotlib import pyplot as plt
```

'results.csv' is in the same directory as this jupyter notebook. So I'll read in the data, and shorten the column name.
```python
results = pd.read_csv("results.csv") #read data into pandas dataframe
results = results.rename(columns = {'movie_or_TV_name' : 'name'})
#rename column

results.head()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>actor</th>
      <th>name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Owen Wilson</td>
      <td>Haunted Mansion,Secret Headquarters,Paint,Marr...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Craig Good</td>
      <td>White Rabbit,Has Been,Cars,Toy Story</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Brian Fee</td>
      <td>Tracy,Cars</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Kathy Coates</td>
      <td>Cars</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Teresa Ganzel</td>
      <td>The Push,Home Economics,A Beauty &amp; The Beast C...</td>
    </tr>
  </tbody>
</table>
</div>


I will create a function, that can read in a dataframe like above, and output a sorted dataframe, where the top elements are the titles that have the most actors in common.
```python
def get_sorted_df(df):
  '''
  df has two columns: actor and name. actor is a string. name is a 
  pandas series containing strings for titles of media for their
  respective actor. Return a sorted dataframe, in descending order, of
  titles with the most actors in common among each other.
  '''

    df['name'] = df[['name']].apply(lambda x: x.str.split(','))
    #change from pandas series to python list, split by commas
    
    counts = {} #dictionary: key is title name, value is counts
    #for number of actors (from this df) in them

    for index, row in df.iterrows(): #iterate over rows of df
        for name in row['name']: #iterate over list of names for each row
            if name in counts: #update counts
                counts.update({name: counts[name] + 1})
            else: #add name to dictionary if new
                counts.update({name: 1})
                
                
    sorted_keys = sorted(counts, key = counts.get, reverse = True)
    #list of titles in decreasing order, sorted by counts

    sorted_counts = {} #dictionary, keys is title names, 
    #values is counts
    for name in sorted_keys: #dict is created with the desired order
        sorted_counts[name] = counts[name]
        
    return pd.DataFrame({'name' : sorted_counts.keys(),
                        'number of shared actors' : sorted_counts.values()})
    #return df using keys and values of our sorted dictionary
```


```python
sorted_df = get_sorted_df(results)
```

## Results

So here are the top 10 movies, with the most shared actors from the movie 'Cars'
```python
sorted_df.head(10)
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>number of shared actors</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Cars</td>
      <td>110</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A Bug's Life</td>
      <td>25</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Monsters University</td>
      <td>24</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Monsters Inc.</td>
      <td>23</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Mater's Tall Tales</td>
      <td>23</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Toy Story 3</td>
      <td>20</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Cars 3</td>
      <td>19</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Tales from Radiator Springs</td>
      <td>17</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Cars 2</td>
      <td>17</td>
    </tr>
    <tr>
      <th>9</th>
      <td>WALL·E</td>
      <td>17</td>
    </tr>
  </tbody>
</table>
</div>

A lot of Pixar movies! I've already seen these so I'll have to look deeper in the dateframe to find a movie I haven't seen!

That concludes this blog going over webscraping on IMDB and analyzing the results for movie recommendations.

Link for the GitHub Repository. https://github.com/mmaemura/IMDB_Scraper

