---
layout: post
title: Blog Post 6
---

In this blog post, I will implement a fake news machine learning model using Tensorflow and the Fuctional API.

'Fake news' has become prevalent in our lives, so how possible is it to train a machine to detect fake news?

# Acquiring the Data

The original data comes from Kaggle, but has been cleaned up slightly in the link we have below.


```python
import tensorflow as tf
import numpy as np
```


```python
import pandas as pd
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
train_df = pd.read_csv(train_url)
```


```python
train_df.shape #22449 news articles in our training set
```




    (22449, 4)




```python
train_df.head()
```





  <div id="df-fb4f48b7-065d-49cf-95cb-44ef4c89abc5">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-fb4f48b7-065d-49cf-95cb-44ef4c89abc5')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-fb4f48b7-065d-49cf-95cb-44ef4c89abc5 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-fb4f48b7-065d-49cf-95cb-44ef4c89abc5');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>




We observe the each row has an id, the title for the news article, the text for the article, and whether or not the news is deemed as 'fake' (which is determined by the authors of the research paper who gathered the data).

# Make a Dataset

Next we will make a Tensorflow Dataset to hold our data. This Dataset can have two inputs (the article title and article text) and one output (is it fake). We will also remove 'stopwords' from our data, which are common words that don't really add meaning, like


```python
import nltk
nltk.download('stopwords')
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Unzipping corpora/stopwords.zip.
    




    True




```python
from nltk.corpus import stopwords

def make_dataset(df):
  #remove stopwords
  stop = stopwords.words('english')

  df['title'] = df['title'].str.lower()
  df['text'] = df['text'].str.lower()

  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  #make dataset
  data = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : df[["title"]], #2 inputs
            "text" : df[["text"]]
        }, 
        {
            "fake" : df[["fake"]]
        }
    )
  )
  data = data.batch(100)
  return data
```

After making the Dataset, we will take 20% of our data and use it as a validation set, which will be useful to evaluate our machine learning models later on.


```python
data = make_dataset(train_df)

data = data.shuffle(buffer_size = len(data)) #shuffle the data

train_size = int(0.8*len(data)) #20% of training data for validation set

train = data.take(train_size)
val   = data.skip(train_size)

len(train), len(val) #number of entries (in batch sizes)
```




    (180, 45)



Next, we will look at our model's 'base rate' which is the model that always guess the most common label (which in this case is True, i.e 'real' news article). We observe that our base rate is 52.3%


```python
#Base Rate
train_df.shape[0], train_df['fake'].sum(), train_df['fake'].sum() / train_df.shape[0]

```




    (22449, 11740, 0.522963160942581)



# Create Models

So which part of the article, title or text (or both), is the most important when determining whether an article is fake or not. Let's create all three versions to find out!

## Model 1: title only

We'll being to create a TensorFlow text-learning model using the Functional API.


```python
# pip install tensorflow==2.4 #maybe necessary
```


```python
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
import re
import string
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import losses
```

We need to begin the process of vectorization, which is the process of turing text in numbers, which is what the machine wants.

The components in TextVectorization() involve 'standardizing' words which turns them all lowercase and removes puncuation. This also enforces a max amount of unique words and a max amount of characters that the title can be. 


```python
size_vocabulary = 2000 #only consider this many words

def standardization(input_data):
    lowercase = tf.strings.lower(input_data) #turn words into all lower case
    no_punctuation = tf.strings.regex_replace(lowercase, #remove puncuation marks
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```


```python
vectorize_title = TextVectorization( #vectorization layer
    standardize=standardization,
    max_tokens=size_vocabulary,
    output_mode='int',
    output_sequence_length=500) 

vectorize_title.adapt(train.map(lambda x, y: x["title"])) #we must adapt our training data
```

After building the vectorization layers, we construct the layers of our machine learning model. The 'Embedding' layer takes our vectorized titles and puts them in a vector space that can place similar words together or form patterns in the direction between words, which is useful as an intermediate step. 'Dropout' is useful to combat overfitting. 'Pooling' layers are useful to get a bird's eye view of the data, rather than patterns that are near each other. Then, 'Dense' is good to gather data together in the final steps.


```python
title_input = keras.Input( #input layer
    shape = (1,),
    name = 'title',
    dtype = 'string'
)

title_features = vectorize_title(title_input)
title_features = layers.Embedding(size_vocabulary, 10, name = "title_embedding")(title_features) #10 dimension embedding layer
title_features = layers.Dropout(0.2)(title_features) #drop out 20% of data
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

output = layers.Dense(2, name = 'fake')(title_features) #2 for fake or not fake

model1 = keras.Model(
    inputs = title_input,
    outputs = output
)
```

The 'output' layer is the final layer with 2 components. We need two, one for fake and the other for not fake.


```python
model1.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     title_embedding (Embedding)  (None, 500, 10)          20000     
                                                                     
     dropout (Dropout)           (None, 500, 10)           0         
                                                                     
     global_average_pooling1d (G  (None, 10)               0         
     lobalAveragePooling1D)                                          
                                                                     
     dropout_1 (Dropout)         (None, 10)                0         
                                                                     
     dense (Dense)               (None, 32)                352       
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________
    

Next, we finally train the model, using our validation Dataset from before and using 10 epochs


```python
model1.compile(optimizer = "adam", #compile the model
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = model1.fit(train, #train the model
                    validation_data=val,
                    epochs = 10, 
                    verbose = True)
```

    Epoch 1/10
    

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)
    

    180/180 [==============================] - 4s 7ms/step - loss: 0.6916 - accuracy: 0.5175 - val_loss: 0.6873 - val_accuracy: 0.5367
    Epoch 2/10
    180/180 [==============================] - 1s 5ms/step - loss: 0.6788 - accuracy: 0.6033 - val_loss: 0.6531 - val_accuracy: 0.7180
    Epoch 3/10
    180/180 [==============================] - 1s 5ms/step - loss: 0.5947 - accuracy: 0.7790 - val_loss: 0.5065 - val_accuracy: 0.8418
    Epoch 4/10
    180/180 [==============================] - 1s 6ms/step - loss: 0.4307 - accuracy: 0.8533 - val_loss: 0.3443 - val_accuracy: 0.8940
    Epoch 5/10
    180/180 [==============================] - 1s 5ms/step - loss: 0.3132 - accuracy: 0.8885 - val_loss: 0.2793 - val_accuracy: 0.8944
    Epoch 6/10
    180/180 [==============================] - 1s 5ms/step - loss: 0.2528 - accuracy: 0.9058 - val_loss: 0.2158 - val_accuracy: 0.9224
    Epoch 7/10
    180/180 [==============================] - 1s 5ms/step - loss: 0.2151 - accuracy: 0.9180 - val_loss: 0.1900 - val_accuracy: 0.9264
    Epoch 8/10
    180/180 [==============================] - 1s 5ms/step - loss: 0.1950 - accuracy: 0.9252 - val_loss: 0.1774 - val_accuracy: 0.9342
    Epoch 9/10
    180/180 [==============================] - 1s 5ms/step - loss: 0.1783 - accuracy: 0.9325 - val_loss: 0.1560 - val_accuracy: 0.9434
    Epoch 10/10
    180/180 [==============================] - 1s 5ms/step - loss: 0.1637 - accuracy: 0.9385 - val_loss: 0.1456 - val_accuracy: 0.9433
    

The results look good! Lets plot the accuracies and see how we did.


```python
from matplotlib import pyplot as plt

def plot_history(history, title = "type of input"):
  plt.plot(history.history["accuracy"], label = "training")
  plt.plot(history.history["val_accuracy"], label = "validation")
  plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
  plt.title(f"{title} as input")
  plt.legend()
```

We achieve a accuracy of about 94% on the validation data. And this is just from using the title!


```python
plot_history(history, title = "Title")
```


    
![png]/images/(output_29_0.png)
    


## Model 2: only text

Now we will create a different model which only looks at the texts of the article. This process is essentially the same, except we're just changing some variable names and 'names', so we will go through this quickly.


```python
vectorize_text = TextVectorization( #same as title model
    standardize=standardization,
    max_tokens=size_vocabulary,
    output_mode='int',
    output_sequence_length=500) #length 500

vectorize_text.adapt(train.map(lambda x, y: x["text"]))

text_input = keras.Input(
    shape = (1,),
    name = 'text',
    dtype = 'string'
)

text_features = vectorize_text(text_input) #same as title model
text_features = layers.Embedding(size_vocabulary, 10, name = "text_embedding")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)

output = layers.Dense(2, name = 'fake')(text_features)

model2 = keras.Model(
    inputs = text_input,
    outputs = output
)
```


```python
model2.summary()
```

    Model: "model_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization_1 (TextV  (None, 500)              0         
     ectorization)                                                   
                                                                     
     text_embedding (Embedding)  (None, 500, 10)           20000     
                                                                     
     dropout_2 (Dropout)         (None, 500, 10)           0         
                                                                     
     global_average_pooling1d_1   (None, 10)               0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_3 (Dropout)         (None, 10)                0         
                                                                     
     dense_1 (Dense)             (None, 32)                352       
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________
    


```python
model2.compile(optimizer = "adam", #compile the model
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = model2.fit(train, #train the model
                    validation_data=val,
                    epochs = 10, 
                    verbose = True)
```

    Epoch 1/10
    

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)
    

    180/180 [==============================] - 3s 11ms/step - loss: 0.6362 - accuracy: 0.6979 - val_loss: 0.4786 - val_accuracy: 0.9072
    Epoch 2/10
    180/180 [==============================] - 2s 10ms/step - loss: 0.3152 - accuracy: 0.9276 - val_loss: 0.2102 - val_accuracy: 0.9542
    Epoch 3/10
    180/180 [==============================] - 2s 10ms/step - loss: 0.1734 - accuracy: 0.9574 - val_loss: 0.1494 - val_accuracy: 0.9631
    Epoch 4/10
    180/180 [==============================] - 2s 11ms/step - loss: 0.1286 - accuracy: 0.9691 - val_loss: 0.1076 - val_accuracy: 0.9747
    Epoch 5/10
    180/180 [==============================] - 2s 10ms/step - loss: 0.1082 - accuracy: 0.9733 - val_loss: 0.0983 - val_accuracy: 0.9747
    Epoch 6/10
    180/180 [==============================] - 2s 10ms/step - loss: 0.0901 - accuracy: 0.9777 - val_loss: 0.0849 - val_accuracy: 0.9782
    Epoch 7/10
    180/180 [==============================] - 2s 10ms/step - loss: 0.0797 - accuracy: 0.9805 - val_loss: 0.0729 - val_accuracy: 0.9791
    Epoch 8/10
    180/180 [==============================] - 2s 10ms/step - loss: 0.0729 - accuracy: 0.9821 - val_loss: 0.0625 - val_accuracy: 0.9879
    Epoch 9/10
    180/180 [==============================] - 2s 10ms/step - loss: 0.0641 - accuracy: 0.9832 - val_loss: 0.0591 - val_accuracy: 0.9825
    Epoch 10/10
    180/180 [==============================] - 2s 10ms/step - loss: 0.0600 - accuracy: 0.9838 - val_loss: 0.0474 - val_accuracy: 0.9893
    

Interesting! The validation accuracy using only the text is similar to the accuracy from only using the title. Texts are a lot longer so we do have more information to use. 98% accuracy is very good. 


```python
plot_history(history, title = 'Text')
```


    
![png](/images/output_35_0.png)
    


## Model 3: title and text

To create a model that uses title and text, we can use the layers we created previously and 'concatenate()' them (this is really cool). Then the 'output' layer is the same again.


```python
main = layers.concatenate([title_features, text_features], axis = 1) #combined model
```


```python
main = layers.Dense(32, activation = 'relu')(main) #same last steps as the other models
output = layers.Dense(2, name = 'fake')(main)
```


```python
model3 = keras.Model(
    inputs = [title_input, text_input], #use both inputs
    outputs = output
)
```

Instead of calling 'model3.summary()' we can call a 'plot_model' to better see the structure of this 2-input model. We observe the symmetry between the two branches and how they come together.


```python
keras.utils.plot_model(model3)
```




    
![png](/images/output_41_0.png)
    



Now, let's train this model, using the same parameters as before.


```python
model3.compile(optimizer = "adam", #compile the model
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = model3.fit(train, #train the model
                    validation_data=val,
                    epochs = 10, 
                    verbose = True)
```

    Epoch 1/10
    180/180 [==============================] - 3s 13ms/step - loss: 0.1675 - accuracy: 0.9726 - val_loss: 0.0399 - val_accuracy: 0.9913
    Epoch 2/10
    180/180 [==============================] - 2s 12ms/step - loss: 0.0341 - accuracy: 0.9914 - val_loss: 0.0240 - val_accuracy: 0.9928
    Epoch 3/10
    180/180 [==============================] - 2s 13ms/step - loss: 0.0240 - accuracy: 0.9928 - val_loss: 0.0249 - val_accuracy: 0.9929
    Epoch 4/10
    180/180 [==============================] - 2s 13ms/step - loss: 0.0226 - accuracy: 0.9934 - val_loss: 0.0201 - val_accuracy: 0.9933
    Epoch 5/10
    180/180 [==============================] - 4s 23ms/step - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.0133 - val_accuracy: 0.9958
    Epoch 6/10
    180/180 [==============================] - 3s 14ms/step - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.0097 - val_accuracy: 0.9980
    Epoch 7/10
    180/180 [==============================] - 2s 12ms/step - loss: 0.0131 - accuracy: 0.9962 - val_loss: 0.0163 - val_accuracy: 0.9944
    Epoch 8/10
    180/180 [==============================] - 2s 12ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.0068 - val_accuracy: 0.9989
    Epoch 9/10
    180/180 [==============================] - 2s 12ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0082 - val_accuracy: 0.9971
    Epoch 10/10
    180/180 [==============================] - 2s 12ms/step - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.0057 - val_accuracy: 0.9984
    

Woah! we got 99% accuracy on the validation data. That's crazy. So it looks like to find fake news, it's best to use both the title and text of the article.


```python
plot_history(history, title = 'Title and Text')
```


    
![png](/images/output_45_0.png)
    


# Model Evaluation

Now let's use our best-performing model, and test it on unseen data.


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(train_url)

test = make_dataset(test_df)
```


```python
model3.evaluate(test) #evaulate the unseen data
```

    225/225 [==============================] - 3s 13ms/step - loss: 0.0051 - accuracy: 0.9986
    




    [0.005118005909025669, 0.9986190795898438]



Nice! We also got 99% accuracy, which is consistent with the training and validation accuracy. 

# Embedding Visualization

We used embedding layers in all three of the models, and I said they place words in a vector space, where the placement of words can have some meaning and patterns. So, lets plot the embeddings and see if we notice any patterns.


```python
weights = model3.get_layer('title_embedding').get_weights()[0]
vocab = vectorize_title.get_vocabulary()
```


```python
weights
```




    array([[-0.00534469,  0.00465663,  0.00772078, ..., -0.00536004,
             0.00089996,  0.00089659],
           [ 0.38389924, -0.35659125, -0.23362653, ...,  0.27147642,
            -0.38527912,  0.3300707 ],
           [ 0.41607404, -0.32075182, -0.20128423, ...,  0.34141856,
            -0.3187928 ,  0.37431926],
           ...,
           [-0.32443038,  0.29917008,  0.3112592 , ..., -0.28592792,
             0.3880022 , -0.3094717 ],
           [ 0.41313756, -0.401259  , -0.3701524 , ...,  0.34346673,
            -0.37965965,  0.39204264],
           [ 0.40569642, -0.45135182, -0.5766739 , ...,  0.45711318,
            -0.47214308,  0.4293159 ]], dtype=float32)




```python
from sklearn.decomposition import PCA
pca = PCA(n_components = 2) #'principle component analysis' to reduce dimensions to be able to plot on 2d plane
weights = pca.fit_transform(weights)
```


```python
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
embedding_df
```





  <div id="df-2035ddde-dda5-4da6-a592-e64eb3323ea8">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>-0.400127</td>
      <td>-0.003804</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>0.661283</td>
      <td>-0.102928</td>
    </tr>
    <tr>
      <th>2</th>
      <td>trump</td>
      <td>0.692857</td>
      <td>-0.141275</td>
    </tr>
    <tr>
      <th>3</th>
      <td>video</td>
      <td>7.164071</td>
      <td>0.279256</td>
    </tr>
    <tr>
      <th>4</th>
      <td>us</td>
      <td>-2.428254</td>
      <td>0.048590</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>funded</td>
      <td>1.220011</td>
      <td>0.008112</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>eye</td>
      <td>-1.566881</td>
      <td>-0.018305</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>extrump</td>
      <td>-1.418554</td>
      <td>0.023058</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>editor</td>
      <td>0.835808</td>
      <td>-0.041268</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>dr</td>
      <td>1.093789</td>
      <td>0.097871</td>
    </tr>
  </tbody>
</table>
<p>2000 rows × 3 columns</p>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-2035ddde-dda5-4da6-a592-e64eb3323ea8')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-2035ddde-dda5-4da6-a592-e64eb3323ea8 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-2035ddde-dda5-4da6-a592-e64eb3323ea8');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 #size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word",
                 title = "Word Embeddings in News Titles")

fig.show()
```

{% include embedding.html %}

On the right half, we see the points "gop", "hillary's", and "obama's" are very close to each other. This closeness could indicate that the GOP does a lot of claims on Hillary Clinton and Barack Obama. Also on that half, though not as extreme right, we can see that "dnc" and "radical" are very close to each other. This may imply that there are many news articles that state that the dnc is radical. 

That concludes this blog. We used Tensorflow to build a model to detect fake news, and we also learned about and visualized word embeddings! Thanks!
