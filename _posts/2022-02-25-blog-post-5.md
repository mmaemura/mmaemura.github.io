---
layout: post
title: Dog Cat Image Classification
---


In this blog post, I will demonstrate model building for image classification using Tensorflow and Keras.

We will want our machine learning model to predict whether an image is of a cat or dog, like those below:

![png](/images/petscats.png)
![png](/images/petsdogs.png)

The first libraries we'll be using:


```python
import tensorflow as tf
import os
from tensorflow.keras import utils
from tensorflow.keras import models, layers, losses
```

# Loading the Data


First we download the data from the internet. Then we use `utils` to construct datasets, which are a Tensorflow object. These datasets hold our images and are a little confusing to work with, but they have lots of good functionality. They're useful if we don't want to keep all of our data loaded in memory.

The downloaded data allows us to easily make train and validation datasets. Then we required more code to create a test dataset, which is every fifth element of the validation dataset.


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    

## Exploring our Dataset.

We'll create a function to ouput a 2x3 plot of images of some of our data, where the first row is of cats and the second row is of dogs


```python
from matplotlib import pyplot as plt

def make_visualization():
  class_names = train_dataset.class_names

  fig, ax = plt.subplots(2,3, figsize=(8,6))
  for images, labels in train_dataset.take(1): #this for loop only loops once
    i = 0
    for j in range(32): #iterate over one batch of 32 images
      row = i // 3
      col = i % 3
      if (class_names[labels[j]] == 'cats') and (i < 3): #if label is cat and we
                                                         #haven't printed 3 cats already
        ax[row,col].imshow(images[j].numpy().astype("uint8"))
        ax[row,col].set_title(class_names[labels[j]])
        i += 1
      if (class_names[labels[j]] == 'dogs') and (i > 2): #if label is dog and we
                                                         #already printed 3 cats
        ax[row,col].imshow(images[j].numpy().astype("uint8"))
        ax[row,col].set_title(class_names[labels[j]])
        i += 1
      ax[row,col].axis("off")
      if i == 6:
        break

```


```python
make_visualization()
```


    
![png](/images/output_9_0.png)
    


This next code improves the efficiency of reading in the data but is not important to explain how it works.


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

Since this is a 2-way classification task and each label has an equal amount, if a model were to only pick 1 of the labels, then that model would have 50% accuracy. This accuracy is what we will consider our 'baseline'.

We want our model to perform better than the baseline in order to be significant at all.


```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()

ndog = 0
ncat = 0
for label in labels_iterator:
  if label == 0:
    ncat += 1
  else:
    ndog +=1

print(f"There are {ncat} images of cats and {ndog} images of dogs")
```

    There are 1000 images of cats and 1000 images of dogs
    

# Model 1

This will be our starter model. When working with images, it's common to alternate between Conv2d and MaxPooling2D layers. Dropout layers add some randomness. Then, Flatten layers turns goes from 2D to 1D, which is important because our last layer Dense(2) needs to be one dimensional with 2 possible outputs, one for dog, one for cat.


```python
#defining
model1 = models.Sequential([
      layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu'),
      layers.MaxPooling2D(pool_size = (2,2)),
      layers.Dropout(0.2),
      layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu'),
      layers.MaxPooling2D(pool_size = (2,2)),
      layers.Dropout(0.2),
      layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'),
      layers.MaxPooling2D(pool_size = (2,2)),
      layers.Flatten(),
      layers.Dense(64, activation = 'relu'),
      layers.Dense(2)
])
```


```python
#compliling
model1.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])
```


```python
#training
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 9s 101ms/step - loss: 31.3841 - accuracy: 0.4895 - val_loss: 0.6926 - val_accuracy: 0.4926
    Epoch 2/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6943 - accuracy: 0.5145 - val_loss: 0.6930 - val_accuracy: 0.5012
    Epoch 3/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.6847 - accuracy: 0.5520 - val_loss: 0.6934 - val_accuracy: 0.5272
    Epoch 4/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6695 - accuracy: 0.5875 - val_loss: 0.6899 - val_accuracy: 0.5347
    Epoch 5/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.6587 - accuracy: 0.5890 - val_loss: 0.6972 - val_accuracy: 0.5421
    Epoch 6/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6288 - accuracy: 0.6320 - val_loss: 0.7063 - val_accuracy: 0.5421
    Epoch 7/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.5969 - accuracy: 0.6600 - val_loss: 0.7219 - val_accuracy: 0.5545
    Epoch 8/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5701 - accuracy: 0.6985 - val_loss: 0.7308 - val_accuracy: 0.5507
    Epoch 9/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5274 - accuracy: 0.7180 - val_loss: 0.8454 - val_accuracy: 0.5594
    Epoch 10/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.4927 - accuracy: 0.7380 - val_loss: 0.8347 - val_accuracy: 0.5483
    Epoch 11/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.4599 - accuracy: 0.7675 - val_loss: 0.9306 - val_accuracy: 0.5582
    Epoch 12/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.4183 - accuracy: 0.7920 - val_loss: 0.9529 - val_accuracy: 0.5532
    Epoch 13/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.3853 - accuracy: 0.8135 - val_loss: 1.0082 - val_accuracy: 0.5953
    Epoch 14/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.3555 - accuracy: 0.8400 - val_loss: 1.0441 - val_accuracy: 0.5743
    Epoch 15/20
    63/63 [==============================] - 6s 82ms/step - loss: 0.3161 - accuracy: 0.8525 - val_loss: 1.2465 - val_accuracy: 0.6077
    Epoch 16/20
    63/63 [==============================] - 7s 102ms/step - loss: 0.2792 - accuracy: 0.8740 - val_loss: 1.3212 - val_accuracy: 0.6052
    Epoch 17/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.2469 - accuracy: 0.8920 - val_loss: 1.5309 - val_accuracy: 0.5916
    Epoch 18/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.2511 - accuracy: 0.8930 - val_loss: 1.4621 - val_accuracy: 0.6139
    Epoch 19/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.2243 - accuracy: 0.9120 - val_loss: 1.6132 - val_accuracy: 0.5916
    Epoch 20/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.2034 - accuracy: 0.9175 - val_loss: 1.5658 - val_accuracy: 0.6101
    


```python
from matplotlib import pyplot as plt
def plot_history(history):
  plt.plot(history.history["accuracy"], label = "training")
  plt.plot(history.history["val_accuracy"], label = "validation")
  plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
  plt.legend()
```


```python
plot_history(history)
```


    
![png](/images/output_19_0.png)
    



Model1 ended with about **62% accuracy** on the validation data. This is about 12% better than our baseline model, which is an okay amount. Additionally, we observe high overfitting with this model-- the training accuracy reached 85%.

# Model 2

We will introduce the process of 'data augmentation', which is when we add modified versions our training images with rotations and reflections. These transformations still results in pictures of dogs and cats, and adding them improves our models' abilities to identify the key features even if they're at different angles.

```python
flips = tf.keras.Sequential([
    layers.RandomFlip() #allows all types of flips
])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(4):
    ax = plt.subplot(2, 2, i + 1)
    augmented_image = flips(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![png](/images/output_22_0.png)
    


The above shows the effects of having a randomly reflection layer in the model


```python
rotates = tf.keras.Sequential([
    layers.RandomRotation(0.5,fill_mode = 'nearest') #fills empty space with nearest pixel value
])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(4):
    ax = plt.subplot(2, 2, i + 1)
    augmented_image = rotates(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![png](/images/output_24_0.png)
    


The above shows the effect of having a randomly rotation layer in the model.
    



```python
model2 = models.Sequential([
      layers.RandomFlip(input_shape = (160,160,3)),
      layers.RandomRotation(factor = .5, fill_mode = 'nearest'),
      layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu'),
      layers.MaxPooling2D(pool_size = (2,2)),
      layers.Dropout(0.2),
      layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu'),
      layers.MaxPooling2D(pool_size = (2,2)),
      layers.Dropout(0.2),
      layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'),
      layers.MaxPooling2D(pool_size = (2,2)),
      layers.Flatten(),
      layers.Dense(64, activation = 'relu'),
      layers.Dense(2)
])
```


```python
model2.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])
```


```python
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 8s 87ms/step - loss: 26.9128 - accuracy: 0.4965 - val_loss: 0.6773 - val_accuracy: 0.5532
    Epoch 2/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6988 - accuracy: 0.5335 - val_loss: 0.6917 - val_accuracy: 0.5359
    Epoch 3/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6816 - accuracy: 0.5420 - val_loss: 0.6865 - val_accuracy: 0.5421
    Epoch 4/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.6791 - accuracy: 0.5460 - val_loss: 0.6747 - val_accuracy: 0.5705
    Epoch 5/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6791 - accuracy: 0.5440 - val_loss: 0.6660 - val_accuracy: 0.5792
    Epoch 6/20
    63/63 [==============================] - 6s 97ms/step - loss: 0.6705 - accuracy: 0.5560 - val_loss: 0.6817 - val_accuracy: 0.5347
    Epoch 7/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6737 - accuracy: 0.5770 - val_loss: 0.6646 - val_accuracy: 0.5730
    Epoch 8/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6625 - accuracy: 0.5725 - val_loss: 0.6604 - val_accuracy: 0.5817
    Epoch 9/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6605 - accuracy: 0.5700 - val_loss: 0.6577 - val_accuracy: 0.5978
    Epoch 10/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.6631 - accuracy: 0.5745 - val_loss: 0.6605 - val_accuracy: 0.5866
    Epoch 11/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6561 - accuracy: 0.5750 - val_loss: 0.6538 - val_accuracy: 0.6002
    Epoch 12/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.6601 - accuracy: 0.5750 - val_loss: 0.6633 - val_accuracy: 0.5965
    Epoch 13/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.6522 - accuracy: 0.5880 - val_loss: 0.6508 - val_accuracy: 0.6200
    Epoch 14/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6513 - accuracy: 0.5850 - val_loss: 0.6466 - val_accuracy: 0.6213
    Epoch 15/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6576 - accuracy: 0.6005 - val_loss: 0.6674 - val_accuracy: 0.5990
    Epoch 16/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6577 - accuracy: 0.5810 - val_loss: 0.6857 - val_accuracy: 0.5495
    Epoch 17/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6761 - accuracy: 0.5655 - val_loss: 0.6644 - val_accuracy: 0.5891
    Epoch 18/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6395 - accuracy: 0.6150 - val_loss: 0.6583 - val_accuracy: 0.5953
    Epoch 19/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6449 - accuracy: 0.6200 - val_loss: 0.6440 - val_accuracy: 0.6374
    Epoch 20/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6584 - accuracy: 0.6200 - val_loss: 0.6374 - val_accuracy: 0.6250
    


```python
plot_history(history)
```


    
![png](/images/output_27_0.png)
    


We achieve about **62% accuracy** with our second model on the validation set. This is about the same as our first model. But this time, overfitting is much less of a concern this time, though still may have some slight overfit.

# Model 3

Next we will introduce a 'Preprocessing' layer to our model. Since our image tenors can have values up to 255 for RGB values, it may be beneficial to rescale these to values between 0 and 1


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])

model3 = models.Sequential([
      preprocessor,
      layers.RandomFlip(),
      layers.RandomRotation(factor = .2),
      layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', input_shape = (160,160,3)),
      layers.MaxPooling2D(pool_size = (2,2)),
      layers.Dropout(0.2),
      layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu'),
      layers.MaxPooling2D(pool_size = (2,2)),
      layers.Dropout(0.2),
      layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'),
      layers.MaxPooling2D(pool_size = (2,2)),
      layers.Flatten(),
      layers.Dense(64, activation = 'relu'),
      layers.Dense(2)
])

model3.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])

history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 90ms/step - loss: 0.7403 - accuracy: 0.5215 - val_loss: 0.6663 - val_accuracy: 0.6114
    Epoch 2/20
    63/63 [==============================] - 8s 122ms/step - loss: 0.6592 - accuracy: 0.5965 - val_loss: 0.6493 - val_accuracy: 0.6139
    Epoch 3/20
    63/63 [==============================] - 7s 91ms/step - loss: 0.6431 - accuracy: 0.6245 - val_loss: 0.6459 - val_accuracy: 0.6522
    Epoch 4/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6236 - accuracy: 0.6435 - val_loss: 0.6276 - val_accuracy: 0.6312
    Epoch 5/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6213 - accuracy: 0.6535 - val_loss: 0.6393 - val_accuracy: 0.6324
    Epoch 6/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5988 - accuracy: 0.6720 - val_loss: 0.5832 - val_accuracy: 0.6993
    Epoch 7/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6017 - accuracy: 0.6670 - val_loss: 0.5768 - val_accuracy: 0.7005
    Epoch 8/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5781 - accuracy: 0.6985 - val_loss: 0.5935 - val_accuracy: 0.6795
    Epoch 9/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5733 - accuracy: 0.7070 - val_loss: 0.5787 - val_accuracy: 0.7030
    Epoch 10/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5633 - accuracy: 0.7080 - val_loss: 0.5652 - val_accuracy: 0.7129
    Epoch 11/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5683 - accuracy: 0.6965 - val_loss: 0.5757 - val_accuracy: 0.7042
    Epoch 12/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5472 - accuracy: 0.7140 - val_loss: 0.5737 - val_accuracy: 0.6993
    Epoch 13/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5408 - accuracy: 0.7245 - val_loss: 0.5617 - val_accuracy: 0.7364
    Epoch 14/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.5419 - accuracy: 0.7170 - val_loss: 0.5474 - val_accuracy: 0.7290
    Epoch 15/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5409 - accuracy: 0.7280 - val_loss: 0.5497 - val_accuracy: 0.7327
    Epoch 16/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5206 - accuracy: 0.7420 - val_loss: 0.5815 - val_accuracy: 0.7030
    Epoch 17/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5286 - accuracy: 0.7370 - val_loss: 0.5796 - val_accuracy: 0.7092
    Epoch 18/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5181 - accuracy: 0.7475 - val_loss: 0.6306 - val_accuracy: 0.6795
    Epoch 19/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5310 - accuracy: 0.7345 - val_loss: 0.5438 - val_accuracy: 0.7252
    Epoch 20/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5129 - accuracy: 0.7420 - val_loss: 0.5511 - val_accuracy: 0.7191
    


```python
plot_history(history)
```


    
![png](/images/output_31_0.png)
    


We achieve about **71% accuracy** on the validation data with our third model. So this is an additional 9% in accuracy over our first model, a very big improvement! Also, this time, there appears to be no sign of overfitting, as the lines between training and validation are similar.

# Model 4

We will demonstrate the practice of 'Transfer Learning', which is when we utilize an already-trained model by someone else and possibly modify it for a given task.

The code below downloads a well-trained model and converts it to a layer that we can utilize.


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```


```python
model4 = models.Sequential([
      preprocessor,
      layers.RandomFlip(),
      layers.RandomRotation(factor = .2),
      base_model_layer,
      layers.Dropout(0.2),
      layers.Flatten(),
      layers.Dense(64, activation='relu'),
      layers.Dense(2)
])

model4.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])


```


```python
model4.summary()
```

    Model: "sequential_14"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model_3 (Functional)        (None, 160, 160, 3)       0         
                                                                     
     random_flip_11 (RandomFlip)  (None, 160, 160, 3)      0         
                                                                     
     random_rotation_11 (RandomR  (None, 160, 160, 3)      0         
     otation)                                                        
                                                                     
     model_4 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     dropout_19 (Dropout)        (None, 5, 5, 1280)        0         
                                                                     
     flatten_10 (Flatten)        (None, 32000)             0         
                                                                     
     dense_21 (Dense)            (None, 64)                2048064   
                                                                     
     dense_22 (Dense)            (None, 2)                 130       
                                                                     
    =================================================================
    Total params: 4,306,178
    Trainable params: 2,048,194
    Non-trainable params: 2,257,984
    _________________________________________________________________
    


```python
history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 11s 108ms/step - loss: 0.8693 - accuracy: 0.8315 - val_loss: 0.0707 - val_accuracy: 0.9740
    Epoch 2/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.2180 - accuracy: 0.9190 - val_loss: 0.1031 - val_accuracy: 0.9691
    Epoch 3/20
    63/63 [==============================] - 6s 97ms/step - loss: 0.1888 - accuracy: 0.9230 - val_loss: 0.0749 - val_accuracy: 0.9703
    Epoch 4/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.1601 - accuracy: 0.9355 - val_loss: 0.0887 - val_accuracy: 0.9728
    Epoch 5/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.1707 - accuracy: 0.9385 - val_loss: 0.0749 - val_accuracy: 0.9703
    Epoch 6/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.1448 - accuracy: 0.9480 - val_loss: 0.0683 - val_accuracy: 0.9728
    Epoch 7/20
    63/63 [==============================] - 7s 100ms/step - loss: 0.1457 - accuracy: 0.9440 - val_loss: 0.0849 - val_accuracy: 0.9666
    Epoch 8/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.1217 - accuracy: 0.9545 - val_loss: 0.0840 - val_accuracy: 0.9666
    Epoch 9/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.1271 - accuracy: 0.9525 - val_loss: 0.0749 - val_accuracy: 0.9728
    Epoch 10/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.1224 - accuracy: 0.9520 - val_loss: 0.0901 - val_accuracy: 0.9703
    Epoch 11/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.1141 - accuracy: 0.9610 - val_loss: 0.1065 - val_accuracy: 0.9691
    Epoch 12/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.1144 - accuracy: 0.9590 - val_loss: 0.0876 - val_accuracy: 0.9752
    Epoch 13/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.1252 - accuracy: 0.9505 - val_loss: 0.0670 - val_accuracy: 0.9802
    Epoch 14/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.1101 - accuracy: 0.9575 - val_loss: 0.0817 - val_accuracy: 0.9728
    Epoch 15/20
    63/63 [==============================] - 6s 97ms/step - loss: 0.1069 - accuracy: 0.9595 - val_loss: 0.0942 - val_accuracy: 0.9752
    Epoch 16/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.1048 - accuracy: 0.9630 - val_loss: 0.0754 - val_accuracy: 0.9765
    Epoch 17/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.0933 - accuracy: 0.9645 - val_loss: 0.0721 - val_accuracy: 0.9814
    Epoch 18/20
    63/63 [==============================] - 6s 97ms/step - loss: 0.0878 - accuracy: 0.9640 - val_loss: 0.0652 - val_accuracy: 0.9728
    Epoch 19/20
    63/63 [==============================] - 6s 97ms/step - loss: 0.1018 - accuracy: 0.9595 - val_loss: 0.0830 - val_accuracy: 0.9653
    Epoch 20/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.0915 - accuracy: 0.9635 - val_loss: 0.0884 - val_accuracy: 0.9678
    


```python
plot_history(history)
```


    
![png](/images/output_38_0.png)
    


With the forth model we achieve about **97% accuracy** on the validation set. This is about an addition 35% accuracy than our first model (amazing!). This training accuracy does not exceed the validation accuracy so we do not suspect overfitting.

# Score on Test Data

Now that we're done modeling, we evaluate our model with images that it's never even seen- from our test dataset that we made at the beginning.


```python
model4.evaluate(test_dataset)
```

    6/6 [==============================] - 1s 68ms/step - loss: 0.0733 - accuracy: 0.9635
    




    [0.07329025864601135, 0.9635416865348816]



We got an accuracy of 96.3% on the test data! That's so good!.

That ends this blog. We went over image classification using data augmentation and transfer learning.

Here are some resources used to make this blog: 

https://www.philchodrow.com/PIC16B//posts/blog-post-5

https://www.tensorflow.org/tutorials/images/transfer_learning
